{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "id": "46f299b78c180a68",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data Exploration:",
   "id": "fcb5a7c6f81dbaab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../data/raw/train.csv')\n",
    "df.head()                       # returns the first 5 rows of pandas DataFrame 'df'"
   ],
   "id": "2c93347d53adb72",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.shape",
   "id": "2b3cfa52945ebfa0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "*Note.* &nbsp;&nbsp;&nbsp;&nbsp; We have 120 000 tuples or a DataFrame with 120 000 rows and 3 columns for `Class Index `, `Title`, and `Description`.",
   "id": "61d6f2157d3e0f1d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.info()",
   "id": "7651dd176f25a84e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Then we do not have any missing values (all tuples have max info).",
   "id": "fc57b9fd3e55bf0f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sns.countplot(df['Class Index'])\n",
    "# plt.xlabel('Class')                 # NOTE: DO NOT run again (takes forever)"
   ],
   "id": "2aaab115038bb2f1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Then also notice that we do not have a class imbalance (equal representation in data)",
   "id": "53dde4a8b1340432"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data Preprocessing:",
   "id": "4cd48134e54c9289"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "PRE_TRAINED_MODEL_NAME = 'bert-base-cased'",
   "id": "e0a51359eba2f7e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Here, `bert-base-cased` refers to the base **BERT** model trained on *cased* (upper and lowercase texts) where as `bert-base-uncased` refers to the base BERT model trained on only lowercase texts. The cased version is recommended.",
   "id": "33cab8759988cfa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "\n",
    "sample_txt = 'I am excited for this semester at the University of Florida. Go Gators!'\n",
    "\n",
    "tokens = tokenizer.tokenize(sample_txt)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(f\"Sentence:  {sample_txt}\")\n",
    "print(f\"Tokens:    {tokens}\")\n",
    "print(f\"Token IDS: {token_ids}\")"
   ],
   "id": "290bd641285b14c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "*Special Tokens*",
   "id": "16fafa6812a989c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "tokenizer.sep_token, tokenizer.sep_token_id",
   "id": "99132dd2b752764d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "&nbsp;&nbsp;&nbsp;&nbsp;&rarr; Marker for ending of a sentence.",
   "id": "8503ab9b89db1d69"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "tokenizer.cls_token, tokenizer.cls_token_id",
   "id": "4a30d5b66eea092",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "&nbsp;&nbsp;&nbsp;&nbsp;&rarr; Marker for classification added to the start of a sentence.",
   "id": "85b546fe6532b097"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "tokenizer.pad_token, tokenizer.pad_token_id",
   "id": "2216b2fe73563f8f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "&nbsp;&nbsp;&nbsp;&nbsp;&rarr; Padding token.",
   "id": "70e07300b002b756"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "tokenizer.unk_token, tokenizer.unk_token_id",
   "id": "6d85856c5688053f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "&nbsp;&nbsp;&nbsp;&nbsp;&rarr; Unknown token (word or subword not present in `bert-base-cased` original training dataset).",
   "id": "d252228cf3a3c987"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "encoding = tokenizer.encode_plus(\n",
    "    sample_txt,\n",
    "    max_length=32,\n",
    "    add_special_tokens=True,    # referring to adding '[CLS]' and '[SEP]'\n",
    "    return_token_type_ids=False,\n",
    "    padding='max_length',\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='pt',         # return PyTorch tensors\n",
    ")\n",
    "\n",
    "encoding.keys()\n",
    "\n",
    "# dict_keys(['input_ids', 'attention_mask'])"
   ],
   "id": "6d0c855f14b96d35",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Notice the presence of *special tokens* after running the `sample_txt` through the `tokenizer` method:\n",
    " + '[CLS]' ~ (101) at the start\n",
    " + '[SEP]' ~ (102) at the end and\n",
    " + '[PAD]' ~ (0) padding indices 17 to 31 with zeros"
   ],
   "id": "9290594bde956d2a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We have a Tensor storing token ids and padded to a length of 32.",
   "id": "6626f3911e2bfac5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(len(encoding['input_ids'][0]))\n",
    "encoding['input_ids'][0]"
   ],
   "id": "b623407a060c4461",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "And we have our attention mask, a binary Tensor used for informing the model which tokens in a sequence should be considered during the **attention mechanism** and which should be ignored.",
   "id": "d5ea5bb9a58fd2d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(len(encoding['attention_mask'][0]))\n",
    "encoding['attention_mask'][0]"
   ],
   "id": "a068f10bc781b16e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Inversing tokenization (text -> token ids), we return to our original text-form sequence of tokens.",
   "id": "3180810ef76d864d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "tokenizer.convert_ids_to_tokens(encoding['input_ids'][0])",
   "id": "23f9f9894f2eb1f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "*Choosing Sequence Length*",
   "id": "1dd4b996094a78e5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Here, `token_lens` is used to store the ordered lengths of descriptions from our training data.",
   "id": "1ae55ddfdf27e8fb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "token_lens = []\n",
    "\n",
    "for txt in df['Description']:\n",
    "    tokens = tokenizer.encode(txt, max_length=512, truncation=True)\n",
    "    token_lens.append(len(tokens))"
   ],
   "id": "1bd096da2832adb7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Plotting the distribution,",
   "id": "522ab604a3b48e7b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sns.displot(token_lens)\n",
    "plt.xlim([0, 275])\n",
    "plt.xlabel('Token count')"
   ],
   "id": "f8c2aeaa3fd15cd0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We notice that `Description`s are between 40 and 50 tokens on average and upwards of 150 tokens captures nearly all `Description`s. Therefore we choose a `MAX_LEN` of 150 to balance accuracy and efficiency.",
   "id": "bec7a8a502e96494"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "MAX_LEN = 150",
   "id": "566f8246ccbd7fa7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "*Creating custom Dataset class*",
   "id": "f7805ddb4f170a98"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NewsArticlesDataset(Dataset):\n",
    "\n",
    "    def __init__(self, descriptions, targets, tokenizer, max_len):\n",
    "        self.descriptions = descriptions    # array of inputs\n",
    "        self.targets = targets              # array of outputs\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.descriptions)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        description = str(self.descriptions[item])\n",
    "        target = self.targets[item]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            description,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'description_text': description,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding ['attention_mask'].flatten(),\n",
    "            'targets': torch.tensor(target, dtype=torch.long)\n",
    "        }"
   ],
   "id": "d599e8a992c046e3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "*Note.* &nbsp;&nbsp;&nbsp;&nbsp; Since our `NewsArticleDataset` class inherits from the `Dataset` class of `torch.utils.data`, note that this class requires the following methods:\n",
    "+ `__len__(self)`: this method returns the total number of samples in the corresponding dataset.\n",
    "+ `__getitem__(self, item)`: this method enables indexing into the corresponding dataset using `item` parameter, allowing for retrieval of specific samples and their corresponding label (or target)."
   ],
   "id": "ec0f568c11d10335"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "*Splitting the data*",
   "id": "b5765c6cc4fe2adb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# df_train, df_test = train_test_split(\n",
    "#     df,\n",
    "#     test_size=0.1,\n",
    "#     random_state=RANDOM_SEED\n",
    "# )\n",
    "#\n",
    "# df_val, df_test = train_test_split(\n",
    "#     df_test,\n",
    "#     test_size=0.5,\n",
    "#     random_state=RANDOM_SEED\n",
    "# )"
   ],
   "id": "48027a9e01c779f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "However, our datasets have been (conveniently) split into `train.csv` and `test.csv` by the file provider.",
   "id": "2c9e3794b0eb86fc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv('../data/raw/train.csv')\n",
    "df_test = pd.read_csv('../data/raw/test.csv')\n",
    "\n",
    "train_rows = len(df_train)\n",
    "test_rows = len(df_test)\n",
    "total_rows = train_rows + test_rows\n",
    "\n",
    "train_ratio = train_rows / total_rows\n",
    "test_ratio = test_rows / total_rows\n",
    "\n",
    "print(f\"Train-Test data split: {train_ratio * 100}%, {test_ratio * 100}%\")"
   ],
   "id": "be5176a894eec1fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# extract ~ 5% of training data for validation (for fine-tuning hyperparams)\n",
    "df_train, df_val = train_test_split(\n",
    "    df_train,\n",
    "    test_size=0.05,\n",
    "    random_state=RANDOM_SEED,\n",
    ")\n",
    "\n",
    "train_rows = len(df_train)\n",
    "val_rows = len(df_val)\n",
    "test_rows = len(df_test)\n",
    "\n",
    "total_rows = train_rows + val_rows + test_rows\n",
    "\n",
    "train_ratio = train_rows / total_rows\n",
    "val_ratio = val_rows / total_rows\n",
    "test_ratio = test_rows / total_rows\n",
    "\n",
    "print(f\"Train-Test data split: {train_ratio * 100}%, {val_ratio * 100}%, {test_ratio * 100}%\")"
   ],
   "id": "c60728ede9f77bca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "*Recall*. &nbsp;&nbsp;&nbsp;&nbsp; ML Process can be broken in the following stages:\n",
    "1. Training (where model learns patterns, relationships, and parameters from training data)\n",
    "2. Validation (used for fine-tuning of hyperparameters and selecting best performing model architecture)\n",
    "3. Testing (dataset independent of training and validation datasets; purpose of providing unbiased evaluation of the final selected model's performance on \"unknown\" data, a simulation for real-world use)"
   ],
   "id": "945b500b3063a370"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Original class ranges:\")\n",
    "print(f\"Train: {df_train['Class Index'].min()} to {df_train['Class Index'].max()}\")\n",
    "print(f\"Val: {df_val['Class Index'].min()} to {df_val['Class Index'].max()}\")\n",
    "print(f\"Test: {df_test['Class Index'].min()} to {df_test['Class Index'].max()}\")\n",
    "\n",
    "df_train['Class Index'] = df_train['Class Index'] - 1\n",
    "df_val['Class Index'] = df_val['Class Index'] - 1\n",
    "df_test['Class Index'] = df_test['Class Index'] - 1\n",
    "\n",
    "print(\"\\nConverted to 0-indexed:\")\n",
    "print(f\"Train: {df_train['Class Index'].min()} to {df_train['Class Index'].max()}\")\n",
    "print(f\"Val: {df_val['Class Index'].min()} to {df_val['Class Index'].max()}\")\n",
    "print(f\"Test: {df_test['Class Index'].min()} to {df_test['Class Index'].max()}\")"
   ],
   "id": "8f9aa39ff8bad245",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "*Note.* &nbsp;&nbsp;&nbsp;&nbsp; The above conversion is required since we are using *Cross Entropy* loss which expects targets in the range [0, `n_classes` - 1].",
   "id": "543664a590378538"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "*Data Loaders*",
   "id": "a199d70caa5e6fc2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "    ds = NewsArticlesDataset(\n",
    "        descriptions=df['Description'].to_numpy(),\n",
    "        targets=df['Class Index'].to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len\n",
    "    )\n",
    "\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=0               # NOTE: Causes errors on Windows unless zero\n",
    "    )"
   ],
   "id": "79a44d8861f451e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "*Note.* &nbsp;&nbsp;&nbsp;&nbsp; PyTorch's `DataLoader` module has a well-known issue where the `num_workers` parameter must be set to zero on Windows devices, otherwise breaking errors ensue.",
   "id": "868f9426f949445d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"
   ],
   "id": "7e4ad114e123adc0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Taking a closer look at an example batch from our training data loader,",
   "id": "28075771cb050407"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data = next(iter(train_data_loader))\n",
    "\n",
    "print(\"Keys in batch:\", data.keys())\n",
    "print(\"Input IDs shape:\", data['input_ids'].shape)\n",
    "print(\"Attention mask shape:\", data['attention_mask'].shape)\n",
    "print(\"Targets shape:\", data['targets'].shape)"
   ],
   "id": "781afa1e21a9e2a9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Topic Classifier Class**",
   "id": "d1531cea7b7e8add"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ],
   "id": "2b81148adc8d7b45",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "*Recall*. &nbsp;&nbsp;&nbsp;&nbsp; `PRE_TRAINED_MODEL_NAME` corresponds with `bert-base-cased`.",
   "id": "a50416a95d170c3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "last_hidden_state, pooled_output = bert_model(\n",
    "    input_ids=encoding['input_ids'],\n",
    "    attention_mask=encoding['attention_mask'],\n",
    "    return_dict=False                           # used to return a tuple\n",
    ")"
   ],
   "id": "a015c64a9f1a862",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Here, `last_hidden_state` is a sqeuence of hidden states of the last layer of the model. The `pooled_output` is obtained by applying the *BertPooler* on `last_hidden_state`.",
   "id": "1ec6e5e6c5eb2ed7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "last_hidden_state.shape",
   "id": "c31de479b5c504a4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "That is we have a tensor 2D (32 x 768) tensor storing 768 hidden states for each of the 32 tokens (limited to 32 previously). We can verify the number of hidden states or hidden units in the feedforward networks with the following:",
   "id": "45ad1a2fefc034bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "bert_model.config.hidden_size",
   "id": "796dba89ebf7018",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Lastly, `pooled_output` acts as a summary of the content after passing through the **BERT** model.",
   "id": "5bb78c71a86e43e9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "pooled_output.shape",
   "id": "1e04abe9760d5c21",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "*Classifier*",
   "id": "dae5fdec31998a80"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch import nn\n",
    "\n",
    "class TopicClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, n_classes):\n",
    "        super(TopicClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "        self.drop = nn.Dropout(p=0.5) # probability of dropping\n",
    "        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _, pooled_output = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=False\n",
    "        )\n",
    "\n",
    "        output = self.drop(pooled_output)\n",
    "\n",
    "        return self.out(output)"
   ],
   "id": "1802f98fb21f6348",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This class is compromised of two components:\n",
    "1. `def __init__(self, n_classes)`:\n",
    "    + `self.bert` represents our pretrained model from which we base the transfer learning off of. BERT provides:\n",
    "        + 12 transformer layers (`bert-base`)\n",
    "        + 768-dimensional representations\n",
    "        + pre-learned linguistic patterns, grammars and some world knowledge\n",
    "    + `self.drop` represents the *dropout layer* which prevents overfitting by randomly dropping 30% (since `p=0.3`) of neurons (that is randomly zeroing `pooled_output` values) during training.\n",
    "        + *Note*. &nbsp;&nbsp; process of randomly dropping neurons during training is referred to as **regularization**.\n",
    "    + `self.out` represents the *classification head* providing a single linear layer with input dimension of 768 (corresponding to **BERT**'s hidden size) and an output dimension corresponding to `n_classes` to classified into.\n",
    "2. `def forward(self, input_ids, attention_mask)`:\n",
    "    + takes in a sequence of token ids and an attention mask for a given batch of texts\n",
    "    + processes through BERT (12 transformer layers)\n",
    "    + returns:\n",
    "        + `last_hidden_layer` of form [batch, seq_len, 768] and corresponding to all token representations\n",
    "        + `pooled_output` of the form [batch, 768], this is the [CLS] token with an additional pooling layer (linear layer + tanh used to further refine given representation).\n",
    "    + *Note*. &nbsp;&nbsp; The [CLS] token is specifically designed for sentence-level tasks, so it makes sense to use pooled_output.\n",
    "    + Classification here takes on a linear transformation:\n",
    "        + [batch, 768] -> [batch, n_classes]"
   ],
   "id": "18f7c48b93bc68ab"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "*Creating an Instance of the Model*",
   "id": "b70a82c25ccab755"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "First, we'll move an example batch of training data to the GPU.",
   "id": "61951e4eb7b88dae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = TopicClassifier(4) # for topics of \"sports\", \"business\", \"sci\"/\"tech\" and                           # \"world\"\n",
    "model = model.to(device)"
   ],
   "id": "ce2428a5b0f4bd20",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "input_ids = data['input_ids'].to(device)\n",
    "attention_mask = data['attention_mask'].to(device)\n",
    "\n",
    "print(f\"Input IDs shape: {input_ids.shape}\")\n",
    "print(f\"Attention mask shape: {attention_mask.shape}\")"
   ],
   "id": "ce194e5af5d455f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "To get the predicted probabilities from our trained model, we apply the `softmax` function to the outputs:",
   "id": "bc3878d42230af38"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "F.softmax(model(input_ids, attention_mask), dim=1)"
   ],
   "id": "93fd93ee2c81139b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training",
   "id": "d5f6783b7254722"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "print(total_steps)\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)"
   ],
   "id": "b0b4837b776e0262",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "*Note.* &nbsp;&nbsp;&nbsp;&nbsp; Adaptive Moment Estimation (Adam) combines the best of RMSprop and momentum. In particular, `AdamW` fixes the standard Adam's incorrect application of *weight* to the adaptive learning rate. This is done by decoupling weight decay from gradient-based updates.",
   "id": "1f4d04f537fc32a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Moreover, notice the following hyperparameter:\n",
    "+ `lr=2e-5` represents the \"magic number\" proven to be the best for BERT fine-tuning"
   ],
   "id": "14766ff28d69ce22"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The schedule profile is created with `get_linear_schedule_with_warmup`. The *no warmup* qualifier refers to immediate start of this process at the learning rate of `lr=2e-5`. (Alternatively, using 10 percent of the `total_steps` for a warmup is typical).\n",
    "+ Warmups are most beneficial when training a model from scratch, however for fine-tuning purposes this becomes less critical.\n",
    "    + (Ex.) For 1000 batches / epoch * 10 epochs we have the following.\n",
    "        + *Step 0:* &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $lr = 2e^{-5}$\n",
    "        + &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ...\n",
    "        + *Step 5 000:* &nbsp;&nbsp;&nbsp;&nbsp; $lr = 1e^{-5}$\n",
    "        + &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ...\n",
    "        + *Step 10 000:* &nbsp;&nbsp; $lr = 0$"
   ],
   "id": "294d6d01615c4e3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The *loss function* (`loss_fn`) uses the `CrossEntropyLoss` method of `torch.nn`. This method combines **LogSoftmax** and **NLLLoss** into a single operation.\n",
    "+ *Recall*. The general **softmax** function takes an array of arbitrary real numbers and convert them into a probability distribution over multiple classes.\n",
    "+ **NLLLoss** refers to the \"negative log-likelihood loss\" function which takes in (given through a forward call) log-probabilities of each available class.\n",
    "+ Combination of operations produces single operation which is generally more stable than its subparts."
   ],
   "id": "e71321f4625b9f90"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "*Training Helper Function*",
   "id": "617356eb377285fe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train_epoch(\n",
    "        model,\n",
    "        data_loader,\n",
    "        loss_fn,\n",
    "        optimizer,\n",
    "        device,\n",
    "        scheduler,\n",
    "        n_examples\n",
    "):\n",
    "    model = model.train()   # enables dropout layers\n",
    "                            # enables batch normalization\n",
    "\n",
    "    # initialize tracking\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    # main loop to handle batches\n",
    "    for d in data_loader:\n",
    "        input_ids = d['input_ids'].to(device)\n",
    "        attention_mask = d['attention_mask'].to(device)\n",
    "        targets = d['targets'].to(device)\n",
    "        # NOTE: Moving tensors from CPU to GPU is critical for perf (100x speedup!)\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "        # predictions and loss\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "        # update metrics\n",
    "        correct_predictions += torch.sum(preds == targets)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # backwards pass/backpropagation\n",
    "        loss.backward()         # calculates gradient\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) # gradient clipping\n",
    "        optimizer.step()        # update weights\n",
    "        scheduler.step()        # update leaning rate\n",
    "        optimizer.zero_grad()   # reset gradients for next batch\n",
    "\n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)"
   ],
   "id": "4252323ec6d5aba7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "*Note.* &nbsp;&nbsp;&nbsp;&nbsp; The function `train_epoch` returns the ratio of correct predictions or the model's *accuracy* and the average *loss* encountered across training batches. The use of the built-in function `double` converts correct_predictions to a float datatype.",
   "id": "3661fa40dc2c8c49"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Here, we perform model training through the following steps:\n",
    "1. **Forward Pass:** Here we pass our *training data* (in batches of 16 texts) through the 12 layers of BERT. The result of this is stored in `outputs` storing value-index pairs.\n",
    "2. **Calculate Loss:** The `loss_fn` function takes raw logits (`outputs`) and true labels (`targets`) and computes *cross-entropy* for each sample, averaging across the batch. The parameter `dim=1` passed into the `torch.max` method corresponds with the predictions occurring along the single dimension of classes (\"world\", \"business\", \"sports\", \"science\").\n",
    "3. **Backward Pass (Backpropagation):** This step entails: computing gradients -> clipping these gradients (meaning normalizing to values between 0 and 1) -> updating weights (utilizing `AdamW` algorithm) -> decaying learning rate (via the `scheduler` $2e^{-5} \\rightarrow 0$) -> clearing gradients (o.w. gradients accumulates across batches causing incorrect updates)."
   ],
   "id": "cdc82f235303fb8f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "*Evaluation Helper Function*",
   "id": "e3d613a6655531d7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "    model = model.eval()\n",
    "\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad():   # disables gradient\n",
    "        for d in data_loader:\n",
    "            input_ids = d['input_ids'].to(device)\n",
    "            attention_mask = d['attention_mask'].to(device)\n",
    "            targets = d['targets'].to(device)\n",
    "\n",
    "            # forward pass\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "\n",
    "            # prediction and loss\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "\n",
    "            # update metrics\n",
    "            correct_predictions += torch.sum(preds == targets)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)"
   ],
   "id": "e2e7941c9875b0de",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "*Note.* &nbsp;&nbsp;&nbsp;&nbsp; Evaluation loop iterations are structured to ensure no gradients, meaning that intermediates are discarded immediately. During evaluation, the weights of the model are not to be altered and thus gradient calculation is redundant. Hence the lack of a *backwards pass*.",
   "id": "ddcf5e128b9f35ed"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Here, we perform model evaluation with the following steps:\n",
    "1. **Forward Pass:** Here we pass our *training data* through the 12 layers of BERT. The result of this is stored in `outputs` storing value-index pairs.\n",
    "2. **Calculate Loss:** The `loss_fn` function takes raw logits (`outputs`) and true labels (`targets`) and computes *cross-entropy* for each sample, averaging across the batch. The parameter `dim=1` passed into the `torch.max` method corresponds with the predictions occurring along the single dimension of classes."
   ],
   "id": "d56d5636621b92c3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "*Training Loop*",
   "id": "ebfa0972e37e630c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%%time\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "history = defaultdict(list) # stores accuracy and loss training loop values\n",
    "best_accuracy = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "    print('-' * 10)\n",
    "\n",
    "    # training\n",
    "    train_acc, train_loss = train_epoch(\n",
    "        model,\n",
    "        train_data_loader,\n",
    "        loss_fn,\n",
    "        optimizer,\n",
    "        device,\n",
    "        scheduler,\n",
    "        len(df_train)\n",
    "    )\n",
    "\n",
    "    print(f\"Training   loss {train_loss}\\taccuracy {train_acc}\")\n",
    "\n",
    "    # validation\n",
    "    val_acc, val_loss = eval_model(\n",
    "        model,\n",
    "        val_data_loader,\n",
    "        loss_fn,\n",
    "        device,\n",
    "        len(df_val)\n",
    "    )\n",
    "\n",
    "    print(f\"Evaluation loss {val_loss}\\taccuracy {val_acc}\")\n",
    "\n",
    "    print()\n",
    "\n",
    "    # update (ordered) accuracy/loss metrics\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "\n",
    "    if val_acc > best_accuracy:\n",
    "        torch.save(model.state_dict(), '../models/checkpoints/best_model_state.pt')\n",
    "        best_accuracy = val_acc\n"
   ],
   "id": "dac4df144ceb70ba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "*Note.* &nbsp;&nbsp;&nbsp;&nbsp; When, what is considered a \"magic command\", `%%time` is placed at the very beginning of a cell in Jupyter Notebooks and IPython consoles, it calculates and displays the *wall time* for all the code within that cell to execute. *Wall time* refers to the actual time elapsed, as measured by a clock on the wall, from the start to the end of execution (including any time spent waiting for I/O and other processes).",
   "id": "fbc2c5e94f572f2d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "*Note.* &nbsp;&nbsp;&nbsp;&nbsp; We use `defaultdict(list)` here instead of the standard Python `dict` since the built-in version requires initialization of each key whereas `defaultdict` auto-initializes keys. The purpose of storing ordered accuracy and loss metrics in the variable `history` from each iteration allow for powerful visualization later on down the line.",
   "id": "10edfc689720ce0d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "*Note.* &nbsp;&nbsp;&nbsp;&nbsp; Notice that at the end of each iteration we save the model only if we have a training pass which is more accurate than those already occurred. This reduces unnecessary/inefficient memory usage. Additionally, the use of `state_dict` saves only the model's state, often requiring less memory than other methods like saving the entire model (model state + dependencies).\n",
    "+ To load a model which was saved using `state_dict`,\n",
    "```\n",
    "# Saving:\n",
    "# torch.save(model.state_dict(), 'model.bin')\n",
    "\n",
    "# Loading:\n",
    "model = TopicClassifier(n_classes)\n",
    "model.load_state_dict(torch.load('model.bin')\n",
    "```\n",
    "Finally, we perform this check using the `val_acc` parameter. This is since the training accuracy (`train_acc`) is expected to improve each iteration, whereas the evaluation accuracy (`val_acc`) measures generalization and thus often improves some iterations while worsening for others. Analogously, training loss (`train_loss`) will gradually decrease and conversely, evaluation loss (`val_loss`) is generally continuous and also less interpretable. This leaves evaluation accuracy as the best choice here!\n",
    "+ Additionally, decisions should NOT be made based on the test set performance!"
   ],
   "id": "5a10d8a37f5ed560"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "*Visualization of Results*",
   "id": "f4a38223cba31564"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# convert tensors to numpy arrays helper function\n",
    "def tensor_to_numpy(values):\n",
    "    \"\"\"Convert list of tensors or values to numpy array\"\"\"\n",
    "    result = []\n",
    "    for v in values:\n",
    "        if torch.is_tensor(v):\n",
    "            result.append(v.cpu().numpy() if v.dim() > 0 else v.cpu().item())\n",
    "        else:\n",
    "            result.append(v)\n",
    "    return np.array(result)\n",
    "\n",
    "# create the plots (using converted tensors)\n",
    "_, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# accuracy plot\n",
    "axes[0].plot(tensor_to_numpy(history['train_acc']), label='Train Accuracy', marker='o')\n",
    "axes[0].plot(tensor_to_numpy(history['val_acc']), label='Validation Accuracy', marker='s')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title('Model Accuracy')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# loss plot\n",
    "axes[1].plot(tensor_to_numpy(history['train_loss']), label='Train Loss', marker='o')\n",
    "axes[1].plot(tensor_to_numpy(history['val_loss']), label='Validation Loss', marker='s')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].set_title('Model Loss (OVERFITTING PRESENT!)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "6b7b10030b0ef425",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "*Comments on Results*",
   "id": "8d68fde042806996"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "From the visuals above, we can conclude the following results:\n",
    "1. Training loss: 0.251 (Epoch 1) $\\rightarrow$ 0.009 (Epoch 10)\n",
    "    + Represents a 96% decrease\n",
    "2. Validation loss: 0.192 (Epoch 1) $\\rightarrow$ 0.482 (Epoch 10)\n",
    "    + Represents a 151% increase (symptom of *overfitting*)\n",
    "    + Peak performance occurs during *Epoch 2* (94.17% accuracy)\n",
    "    + Model begins memorizing training data after we reach this peak performance (logical to reduce epochs for future training)\n",
    "\n",
    "Regarding the issue of **overfitting** the training data can be addressed for future training in the following ways:\n",
    "1. *Increasing dropout*. This can be done within the initializer method of the `TopicClassifier` class by changing the value of the `self.drop` parameter (ex. increasing from `p=0.3` to `p=0.5`).\n",
    "    + Generally dropout rates range from [0.2, 0.5], with rates like 0.5 seen for fully connected layers and rates like 0.1 and 0.2 being more suitable for convolutional layers.\n",
    "    + The optimal rate is a hyperparameter that depends on the dataset size and model complexity.\n",
    "    + Higher dropout rates *decrease overfitting* but can slow training/produce **underfitting** while lower rates ofer milder regularization (reduction of overfitting).\n",
    "2. *Reduce learning rate*. Reducing learning rate by a factor of two for instance ($2e^{-5} \\rightarrow 1e{-5}$) within the `optimizer` set up:\n",
    "    ```\n",
    "    optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "    ```\n",
    "3. *Use weight decay*. Introducing *weight decay*, a regularization technique that prevents model **overfitting** by discouraging large weights can be done with the following:\n",
    "    ```\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "    ```\n",
    "4. *Implementing Early Stopping*. Regularization technique used to prevent **overfitting** by monitoring a model's performance on a separate *validation set* during training and halting the training process once performance on the *validation set* starts to degrade. This also saves the model weights (and other state data) from the point of optimal performance.\n",
    "    ```\n",
    "    class EarlyStopping:\n",
    "        patience: int = 3\n",
    "        min_delta: float = 0.001\n",
    "        counter: int = 0\n",
    "        # ...\n",
    "\n",
    "        def __call__(self, val_loss):\n",
    "        \"\"\"dfdfsd\"\"\"\n",
    "\n",
    "        # ...\n",
    "\n",
    "    # ... in training loop:\n",
    "    early_stopping = EarlyStopping(patience=3)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        # ... training code ...\n",
    "\n",
    "        if early_stopping(val_loss):\n",
    "            print(f\"Early stopping triggered at epoch {epoch}\")\n",
    "            break\n",
    "    ```"
   ],
   "id": "842e229b709e5a42"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "After adjusting the dropout rate to `p=0.5` and implementing an `EarlyStopping` class, we enter the training loop, replicated below:",
   "id": "d0a8d5a90d01f609"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping class for preventing model overfitting\"\"\"\n",
    "\n",
    "    def __init__(self, patience=3):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_acc = 0\n",
    "\n",
    "    def __call__(self, current_score, model=None):\n",
    "        \"\"\"Check if training should be stopped\"\"\"\n",
    "        if val_acc > self.best_acc:\n",
    "            self.best_acc = val_acc\n",
    "            self.counter = 0\n",
    "            return False\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "\n",
    "            return False\n"
   ],
   "id": "6f2c043d53e4eac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The initializer method for this class possesses the following attributes:\n",
    "+ `patience`: representing how many epochs to wait after most recent improvement\n",
    "+ `counter` and `best_acc` member variables for storing epochs since last improvement and highest `val_acc` value seen so far, respectively.\n",
    "\n",
    "The caller method:\n",
    "+ Takes in `current_score` referring to the current epoch's accuracy metric\n",
    "+ Updates `best_acc` and resets `counter` as a larger accuracies are seen\n",
    "+ Otherwise, increments `counter` and checks it against `patience` parameter, returning true (and triggering early stop) if exceeding the value of `patience`."
   ],
   "id": "66bfa7cbe5be3274"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "*Preparing GPU for training*",
   "id": "9ddcdef30f4513e5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "*Note*. &nbsp;&nbsp;&nbsp;&nbsp; Only required once before running all previous cells.\n",
   "id": "e18e153ac8741359"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "torch._C._cuda_emptyCache()\n",
    "\n",
    "\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"Memory allocated: {torch.cuda.memory_allocated(0)/1024**3:.2f} GB\")\n",
    "print(f\"Memory reserved: {torch.cuda.memory_reserved(0)/1024**3:.2f} GB\")\n",
    "print(f\"Total memory: {torch.cuda.get_device_properties(0).total_memory/1024**3:.2f} GB\")"
   ],
   "id": "ec667ef3ea9831eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "*Training Loop (version 2)*",
   "id": "1dbe67e247b5cbb6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%%time\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "history = defaultdict(list) # stores accuracy and loss training loop values\n",
    "best_accuracy = 0\n",
    "\n",
    "early_stopping = EarlyStopping(patience=3)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "    print('-' * 10)\n",
    "\n",
    "    # training\n",
    "    train_acc, train_loss = train_epoch(\n",
    "        model,\n",
    "        train_data_loader,\n",
    "        loss_fn,\n",
    "        optimizer,\n",
    "        device,\n",
    "        scheduler,\n",
    "        len(df_train)\n",
    "    )\n",
    "\n",
    "    print(f\"Training   loss {train_loss}\\taccuracy {train_acc}\")\n",
    "\n",
    "    # validation\n",
    "    val_acc, val_loss = eval_model(\n",
    "        model,\n",
    "        val_data_loader,\n",
    "        loss_fn,\n",
    "        device,\n",
    "        len(df_val)\n",
    "    )\n",
    "\n",
    "    print(f\"Evaluation loss {val_loss}\\taccuracy {val_acc}\")\n",
    "\n",
    "    # update (ordered) accuracy/loss metrics\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "\n",
    "    if val_acc > best_accuracy:\n",
    "        torch.save(model.state_dict(), '../models/checkpoints/best_model_state.pt')\n",
    "        best_accuracy = val_acc\n",
    "\n",
    "    # check for early stopping\n",
    "    if early_stopping(val_loss):\n",
    "        print(f\"Early stopping at epoch {epoch + 1}.\\t Best accuracy: {best_accuracy:.4f}\")\n",
    "        break\n",
    "\n",
    "    print()\n"
   ],
   "id": "a12d478b2299c4a2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "*Visualization of Results*",
   "id": "36484c82281766db"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# create the plots (using converted tensors)\n",
    "_, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# accuracy plot\n",
    "axes[0].plot(tensor_to_numpy(history['train_acc']), label='Train Accuracy', marker='o')\n",
    "axes[0].plot(tensor_to_numpy(history['val_acc']), label='Validation Accuracy', marker='s')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title('Model Accuracy')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# loss plot\n",
    "axes[1].plot(tensor_to_numpy(history['train_loss']), label='Train Loss', marker='o')\n",
    "axes[1].plot(tensor_to_numpy(history['val_loss']), label='Validation Loss', marker='s')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].set_title('Model Loss (OVERFITTING REDUCED!)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "7d0be958b1c120b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "*Commentary on Adjustments*",
   "id": "340cd0f26355551b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "After making these adjustments,\n",
    "+ We saved 40% training time (3h 42m vs 6h 7m)\n",
    "+ By stopping before degradation we prevented severe overfitting\n",
    "+ Validation loss increasing, but at a more controlled degree (0.192 $\\rightarrow$ 0.482, or a 151% increase vs 0.192 $\\rightarrow$ 0.420, a 119% increase)\n",
    "\n",
    "Analysis of accuracy and loss plots:\n",
    "1. Accuracy Plot\n",
    "    + Training curve possesses more gradual climb to 99% rather than the sharp jump we had previously\n",
    "    + Validation curve is fairly stable; range of [94.0 - 94.4%]\n",
    "2. Loss Plot\n",
    "    + Training loss also decreases more steadily\n",
    "    + Validation loss increases at a lower rate\n",
    "\n",
    "The key benefit of increasing the *dropout rate*:\n",
    "+ With only 50 percent of neurons active, the training data becomes harder to memorize, forcing a more robust feature learning since the model must focus on recognizing patterns in the data instead of brute memorization"
   ],
   "id": "f67222853b83112"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Testing",
   "id": "db037a5ae3d41733"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_acc, _ = eval_model(\n",
    "    model,\n",
    "    test_data_loader,\n",
    "    loss_fn,\n",
    "    device,\n",
    "    len(df_test)\n",
    ")\n",
    "\n",
    "test_acc.item()"
   ],
   "id": "de93951bb06d89cf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This *test accuracy* score is within 1 percent of the validation accuracy score from our peak performing epoch of the most recent training. Thus, the *test accuracy* reliably predicts our *test accuracy* (and by extension our real-world accuracy).",
   "id": "dd0d52a00940e946"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "*Helper function for extracting predicted probabilities for each text description*",
   "id": "31677f25fb33b049"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_predictions(model, data_loader):\n",
    "    model = model.eval()\n",
    "\n",
    "    description_texts = []\n",
    "    predictions = []\n",
    "    prediction_probs = []\n",
    "    real_values = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            texts = d['description_text']\n",
    "            input_ids = d['input_ids'].to(device)\n",
    "            attention_mask = d['attention_mask'].to(device)\n",
    "            targets = d['targets'].to(device)\n",
    "\n",
    "            # perform forward pass\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "            description_texts.extend(texts)\n",
    "            predictions.extend(preds)\n",
    "            prediction_probs.extend(outputs)\n",
    "            real_values.extend(targets)\n",
    "\n",
    "            # NOTE. We use extend() here since texts, preds, outputs and targets represent sequences themselves whereas we would use append() for adding single items to a list.\n",
    "\n",
    "        predictions = torch.stack(predictions).cpu()\n",
    "        prediction_probs = torch.stack(prediction_probs).cpu()\n",
    "        real_values = torch.stack(real_values).cpu()\n",
    "\n",
    "        return description_texts, predictions, prediction_probs, real_values\n"
   ],
   "id": "c733a3677d413a0e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "*Classificaiton Report*",
   "id": "9b3908cdf5df79d9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_description_texts, y_pred, y_pred_probs, y_test = get_predictions(\n",
    "    model, test_data_loader\n",
    ")\n",
    "\n",
    "class_names = ['World', 'Sports', 'Business', 'Sci/Tech']\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=class_names))"
   ],
   "id": "87ce9f824c4ccb36",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "From the report above,\n",
    "+ The model classifies **Sports** and **World** news articles best while classifying **Business** and **Sci/Tech** articles marginally less accurately."
   ],
   "id": "d1155864b7b97d27"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "*Confusion Matrix*",
   "id": "241179c93f84228a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# theme/config for heatmap\n",
    "sns.set_theme(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
    "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
    "\n",
    "def show_confusion_matrix(confusion_matrix):\n",
    "    hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    hmap.axes.set_yticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n",
    "    hmap.axes.set_xticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n",
    "\n",
    "    plt.ylabel('Actual topic')\n",
    "    plt.xlabel('Predicted topic')\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
    "show_confusion_matrix(df_cm)"
   ],
   "id": "98824f8fed51a18e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "*Note*. &nbsp;&nbsp;&nbsp;&nbsp; Here we use `sns.heatmap` from the `seaborn` library (which is mainly used for creating Python data visualizations (built on top of `matplotlib`)). The heatmap is created of the confusion matrix, a matrix of correct prediction instances. The heatmap confirms the model's marginal difficulty at classifying **Business** and **Sci/Tech** news relative to the other topics.",
   "id": "a14c3e214f39d6a9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Taking a closer look at an example batch from our test data,",
   "id": "846d08abca926ff1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "idx = 6998 # arbitrary int in [0, 7600]\n",
    "\n",
    "description_text = y_description_texts[idx]\n",
    "actual_topic = y_test[idx]\n",
    "pred_df = pd.DataFrame({\n",
    "    'class_names': class_names,\n",
    "    'values': y_pred_probs[idx],\n",
    "})"
   ],
   "id": "35259ae3fea4e9a2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from textwrap import wrap\n",
    "\n",
    "print(\"\\n\".join(wrap(description_text)))\n",
    "print()\n",
    "print(f\"Actual topic: {class_names[actual_topic]}\")"
   ],
   "id": "d96e5ed32b8a1507",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Continuing, we can examine the *confidence* of each topic suggested by the model:",
   "id": "9d8ee5f2c60f014c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sns.barplot(x='values', y='class_names', data=pred_df, orient='h')\n",
    "plt.ylabel('topic')\n",
    "plt.xlabel('probability')\n",
    "plt.xlim([0, 1])"
   ],
   "id": "4e1971f9cc4d5150",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "From manually testing multiple different indices in the range [0, 7600] the model appears nearly 100 percent confident in most cases. Rarely, the model will produce this high confidence for two competing topics which the model considers.",
   "id": "9282207e46ca1130"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
