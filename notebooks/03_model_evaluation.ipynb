{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "a80a716f35bc7116",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "from src.train import eval_model\n",
    "\n",
    "test_acc, _ = eval_model(\n",
    "    model,\n",
    "    test_data_loader,\n",
    "    loss_fn,\n",
    "    device,\n",
    "    len(df_test)\n",
    ")\n",
    "\n",
    "test_acc.item()"
   ],
   "id": "ed0cdd8d208e9867",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This *test accuracy* score is within 1 percent of the validation accuracy score from our peak performing epoch of the most recent training. Thus, the *test accuracy* reliably predicts our *test accuracy* (and by extension our real-world accuracy).\n",
   "id": "eefb93d1bfcce9de"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "*Helper function for extracting predicted probabilities for each text description*\n",
   "id": "ea9160af43db84d7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_predictions(model, data_loader):\n",
    "    model = model.eval()\n",
    "\n",
    "    description_texts = []\n",
    "    predictions = []\n",
    "    prediction_probs = []\n",
    "    real_values = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            texts = d['description_text']\n",
    "            input_ids = d['input_ids'].to(device)\n",
    "            attention_mask = d['attention_mask'].to(device)\n",
    "            targets = d['targets'].to(device)\n",
    "\n",
    "            # perform forward pass\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "            description_texts.extend(texts)\n",
    "            predictions.extend(preds)\n",
    "            prediction_probs.extend(outputs)\n",
    "            real_values.extend(targets)\n",
    "\n",
    "            # NOTE. We use extend() here since texts, preds, outputs and targets represent sequences themselves whereas we would use append() for adding single items to a list.\n",
    "\n",
    "        predictions = torch.stack(predictions).cpu()\n",
    "        prediction_probs = torch.stack(prediction_probs).cpu()\n",
    "        real_values = torch.stack(real_values).cpu()\n",
    "\n",
    "        return description_texts, predictions, prediction_probs, real_values\n"
   ],
   "id": "bf2b2c1d21ad16ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "*Classificaiton Report*",
   "id": "74e415eb4e6230f7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_description_texts, y_pred, y_pred_probs, y_test = get_predictions(\n",
    "    model, test_data_loader\n",
    ")\n",
    "\n",
    "class_names = ['World', 'Sports', 'Business', 'Sci/Tech']\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=class_names))"
   ],
   "id": "780d9dc122bb6c69",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "From the report above,\n",
    "+ The model classifies **Sports** and **World** news articles best while classifying **Business** and **Sci/Tech** articles marginally less accurately."
   ],
   "id": "a93fa9c0699b4daa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "*Confusion Matrix*",
   "id": "81758a685905c9bc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# theme/config for heatmap\n",
    "sns.set_theme(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
    "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
    "\n",
    "def show_confusion_matrix(confusion_matrix):\n",
    "    hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    hmap.axes.set_yticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n",
    "    hmap.axes.set_xticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n",
    "\n",
    "    plt.ylabel('Actual topic')\n",
    "    plt.xlabel('Predicted topic')\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
    "show_confusion_matrix(df_cm)"
   ],
   "id": "4d7e51ea37ea061e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "*Note*. &nbsp;&nbsp;&nbsp;&nbsp; Here we use `sns.heatmap` from the `seaborn` library (which is mainly used for creating Python data visualizations (built on top of `matplotlib`)). The heatmap is created of the confusion matrix, a matrix of correct prediction instances. The heatmap confirms the model's marginal difficulty at classifying **Business** and **Sci/Tech** news relative to the other topics.",
   "id": "f328306a6dbd9d96"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Taking a closer look at an example batch from our test data,",
   "id": "69d166612c205fbe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "idx = 6998 # arbitrary int in [0, 7600]\n",
    "\n",
    "description_text = y_description_texts[idx]\n",
    "actual_topic = y_test[idx]\n",
    "pred_df = pd.DataFrame({\n",
    "    'class_names': class_names,\n",
    "    'values': y_pred_probs[idx],\n",
    "})"
   ],
   "id": "3db4e05e7c024615",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from textwrap import wrap\n",
    "\n",
    "print(\"\\n\".join(wrap(description_text)))\n",
    "print()\n",
    "print(f\"Actual topic: {class_names[actual_topic]}\")"
   ],
   "id": "95716ef3f67e20cb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Continuing, we can examine the *confidence* of each topic suggested by the model:",
   "id": "dab3f49b6933ac6f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sns.barplot(x='values', y='class_names', data=pred_df, orient='h')\n",
    "plt.ylabel('topic')\n",
    "plt.xlabel('probability')\n",
    "plt.xlim([0, 1])"
   ],
   "id": "7170c7f13b8189ec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "From manually testing multiple different indices in the range [0, 7600] the model appears nearly 100 percent confident in most cases. Rarely, the model will produce this high confidence for two competing topics which the model considers.",
   "id": "3e22adde3dfd8c2f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
